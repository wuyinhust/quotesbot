{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Om1MpPijUOGB"
      },
      "source": [
        "# Style transfer\n",
        "\n",
        "In this tutorial, we will create and train a neural model for style transfer. It's a computer vision method used for reproducing an image (so-called content image) with style features of another one (so-called, reference image). With neural style transfer, it becomes possible to imitate painting techniques of famous artists.\n",
        "\n",
        "We will train our neural network on the famous [COCO dataset](http://cocodataset.org/#home) that contains images of different objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTkC8t5unzxs"
      },
      "source": [
        "# Install libraries\n",
        "\n",
        "First, we need to prepare our work environment and install the necessary Python packages. If you're using Google Colab, you already have them, and you can skip next cell.\n",
        "\n",
        "We added strict version requirements for the packages for better reproducibility.\n",
        "\n",
        "Note that these versions of packages will replace already installed ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmoGzDJpCXy3"
      },
      "outputs": [],
      "source": [
        "%pip install -q onnx\n",
        "\n",
        "if 'google.colab' in str(get_ipython()):\n",
        "    print('Skipping pip installation on Google Colab')\n",
        "else:\n",
        "    %pip install -q numpy==1.18.2 opencv-python-headless==4.2.0.32 \\\n",
        "        torch==1.4.0 torchvision==0.5.0 folium==0.2.1 \\\n",
        "        albumentations==0.4.5 tqdm==4.43.0 matplotlib==3.2.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_ujEUJhCXy-"
      },
      "source": [
        "# Import libraries\n",
        "\n",
        "Let's import the libraries we will use in the project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZDysKMmCXy_"
      },
      "outputs": [],
      "source": [
        "from itertools import islice\n",
        "from pathlib import Path\n",
        "import random\n",
        "import urllib.request\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import albumentations as albm\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils import data\n",
        "from torch.nn.utils import spectral_norm\n",
        "import torchvision\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhK6oUMhUOGP"
      },
      "source": [
        "For better reproducibility, it would be useful to set random seeds for the libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrYw9BskUOGP"
      },
      "outputs": [],
      "source": [
        "RANDOM_SEED = 123\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "torch.cuda.manual_seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Flip values for slower training speed, but more determenistic results.\n",
        "torch.backends.cudnn.deterministic = False\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkZH8gxdUOGS"
      },
      "source": [
        "It's recommended to train neural networks on GPU. However, it's possible to train them on a CPU as well. We will use the 0th GPU (`cuda:0`) if a GPU is available and CPU otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKYgAK6jUOGS"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print('device:', DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdhHbnoYwtmX"
      },
      "source": [
        "# Global variables for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-YIXQ_dwtmY"
      },
      "source": [
        "This training notebook uses [COCO dataset](http://cocodataset.org/). The annotations in this dataset belong to the COCO Consortium and are [licensed](http://cocodataset.org/#termsofuse) under a Creative Commons Attribution 4.0 License. Images are part of Flickr and have corresponding licenses. To check license for each image please refer to [the image information](http://images.cocodataset.org/annotations/image_info_unlabeled2017.zip).\n",
        "\n",
        "Let's set the parameters of the dataset and the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F68rYSMyCXzH"
      },
      "outputs": [],
      "source": [
        "COCO_CATEGORY = 'person'  # target category\n",
        "\n",
        "WORK_DIR = Path('.')\n",
        "\n",
        "# Archives to be downloaded\n",
        "COCO_IMAGES_ARCHIVE = WORK_DIR / 'train2017.zip'\n",
        "COCO_ANNOTATIONS_ARCHIVE = WORK_DIR / 'annotations_trainval2017.zip'\n",
        "\n",
        "# Paths where the dataset will be extracted to\n",
        "COCO_ANNOTATIONS_PATH = WORK_DIR / 'annotations/instances_train2017.json'\n",
        "COCO_IMAGES_PATH = WORK_DIR / 'train2017'\n",
        "\n",
        "# How many images use for training\n",
        "DATASET_SIZE = 32000\n",
        "\n",
        "# A content and a style images. You can upload your own.\n",
        "TEST_IMAGE_PATH = WORK_DIR / 'test_image.png'\n",
        "STYLE_IMAGE_PATH = WORK_DIR / 'style_image.png'\n",
        "\n",
        "NUM_TRAINING_STEPS = 10000  # number of steps for training, longer is better\n",
        "LOGGING_FREQUENCY = 250     # log validation every N steps\n",
        "BATCH_SIZE = 16             # number of images per batch\n",
        "NUM_WORKERS = 4             # number of CPU threads available for image preprocessing\n",
        "\n",
        "# This controls input and output resolution for the network.\n",
        "# Lower values lead to worse mask quality, but faster network inference.\n",
        "# Change carefully.\n",
        "INPUT_HEIGHT = 512\n",
        "INPUT_WIDTH = 256\n",
        "\n",
        "# Base number of channels for model. Higher is stronger effect, but slower model\n",
        "MODEL_WIDTH = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "6L0Guwl_UOGZ"
      },
      "outputs": [],
      "source": [
        "LR = 0.01                    # Initial learning rate\n",
        "LR_DECREASING_FACTOR = 0.1   # Factor of decreasing LR by a scheduler\n",
        "WEIGHT_DECAY = 0.0001        # Weight decaying coefficient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXykMfLhUOGc"
      },
      "outputs": [],
      "source": [
        "ONNX_PATH = './style_new.onnx'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0R1-8eMUOGf"
      },
      "source": [
        "The constants below are the weights of the loss functions' components. You can change it for tuning style-identity tradeoff."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdBhypYLUOGf"
      },
      "outputs": [],
      "source": [
        "STYLE_WEIGHT = 1e6              # How strong style should be forced\n",
        "RECONSTRUCTION_WEIGHT = 5e-3    # How strong identity image will be preserved\n",
        "CONSISTENCY_WEIGHT = 5e-3       # As larger value more stable network will be, but weaker effect"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxt9L7cin9h0"
      },
      "source": [
        "Colab's GPU runtimes don't have enough free drive space to store the entire COCO dataset, so we'll have to unpack only the files that we need for training.\n",
        "\n",
        "First of all, we have to download the archives with images and annotations. For convenience, we can use the functions below. It downloads a file but only if we have not already downloaded it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fz9MKd0Swtmj"
      },
      "outputs": [],
      "source": [
        "def download_file(link, filename):\n",
        "    if Path(filename).exists():\n",
        "        return\n",
        "    progress_bar = tqdm(desc=str(filename),\n",
        "                        dynamic_ncols=True, leave=False,\n",
        "                        mininterval=5, maxinterval=30,\n",
        "                        unit='KiB', unit_scale=True,\n",
        "                        unit_divisor=1024)\n",
        "\n",
        "    def update_progress(count, block_size, total_size):\n",
        "        if progress_bar.total is None:\n",
        "            progress_bar.reset(total_size)\n",
        "        progress_bar.update(count * block_size - progress_bar.n)\n",
        "\n",
        "    urllib.request.urlretrieve(link, filename, reporthook=update_progress)\n",
        "    urllib.request.urlcleanup()\n",
        "    progress_bar.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1CUBJrzUOGk"
      },
      "source": [
        "Now we can download archives with images and annotations, and extract the data we need from them. If you encounter message \"Disk is almost full\" on Google Colab, please click the \"ignore\" button."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m8fUM51rUOGl",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "download_file('http://images.cocodataset.org/zips/train2017.zip',\n",
        "              COCO_IMAGES_ARCHIVE)\n",
        "\n",
        "download_file('http://images.cocodataset.org/annotations/annotations_trainval2017.zip',\n",
        "              COCO_ANNOTATIONS_ARCHIVE)\n",
        "with ZipFile(COCO_ANNOTATIONS_ARCHIVE, 'r') as archive:\n",
        "    archive.extractall()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPeDPmbXUOGr"
      },
      "source": [
        "An image ID is a 12-digits number padded with zeros. The name of every image inside the archive with images is an image ID with the `.jpg` extension. We can extract all the IDs by listing the archive and getting the names of images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKWii0lslaJZ"
      },
      "outputs": [],
      "source": [
        "with ZipFile(COCO_IMAGES_ARCHIVE) as archive:\n",
        "    coco_images_list = archive.namelist()\n",
        "all_ids = [int(Path(name).stem) for name in coco_images_list[1:]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwIB35QfUOGu"
      },
      "source": [
        "Now we will sample a fraction of training examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wy4fzqnmUOGv"
      },
      "outputs": [],
      "source": [
        "train_img_ids = random.sample(all_ids, min(len(all_ids), DATASET_SIZE))\n",
        "random.shuffle(train_img_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaSoxDzhwtmu"
      },
      "source": [
        "Now we will extract only the images that we need for training since COCO is pretty large."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "87a1373ae70b4cb3a09f44b2ad35d22a",
            "82a7f264a43b475d9c70750d0f7cf38f",
            "1f6c859a79d8400c8d4bfa51985c2df0",
            "79b5e8c093124858ab43b24d69493170",
            "b7057dbfc7f542e0be08a06ac95b5378",
            "b8156670519649f3b34814769881f4d1",
            "a098227217dc4e4fabd8807980d3df2e",
            "c6b03af9b0014de9a7e981ad391b8a08",
            "6f85be314480419aa274e8f955e695c2",
            "6f0a23b63bc84e8a8c593a8c6d4f791f",
            "7762b5a6a7f64c72b5b4fbb7d3f3af1e"
          ]
        },
        "id": "USQo89SbtIvz",
        "outputId": "5c7641f9-e3e6-4774-8fcc-7913982624b3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/32000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87a1373ae70b4cb3a09f44b2ad35d22a"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Prepare a directory for the images\n",
        "COCO_IMAGES_PATH.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "with ZipFile(COCO_IMAGES_ARCHIVE, 'r') as archive:\n",
        "    for image_id in tqdm(train_img_ids, dynamic_ncols=True, leave=False, mininterval=5, maxinterval=30):\n",
        "        image_name = str(image_id).zfill(12) + '.jpg'\n",
        "        image_path = Path('train2017') / image_name\n",
        "\n",
        "        if Path(image_path).exists():\n",
        "            continue\n",
        "        archive.extract(str(image_path), WORK_DIR)\n",
        "\n",
        "        # resize for faster loading time\n",
        "        image = cv2.imread(str(image_path))\n",
        "        scaled_width = int(INPUT_HEIGHT * image.shape[1] / image.shape[0])\n",
        "        image = cv2.resize(image, (scaled_width, INPUT_HEIGHT),\n",
        "                        interpolation=cv2.INTER_LINEAR)\n",
        "        cv2.imwrite(str(image_path), image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4W-HmChzwtmy"
      },
      "source": [
        "Let's also load an image that we'll use to check the quality of the Style Transfer while we train the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "PyhFbijHlSVD"
      },
      "outputs": [],
      "source": [
        "test_img_bgr = cv2.imread(str(TEST_IMAGE_PATH))\n",
        "TEST_IMG = cv2.cvtColor(test_img_bgr, cv2.COLOR_BGR2RGB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_MwgVx2UOG4"
      },
      "source": [
        "Let's take a look at our test image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkEOGW9XCXzL"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(6, 12))\n",
        "plt.imshow(TEST_IMG)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JRgISkYMCXzQ"
      },
      "source": [
        "# Model\n",
        "\n",
        "Now we can describe the model we will use for style transfer. We will build the model from mainly two types of blocks: residual hourglass-like blocks and separable residual ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIDJjSBEUOG_"
      },
      "source": [
        "Separable residual block. The main idea of separable blocks is splitting a convolution to two parts: a channel-wise and a depth-wise one. They are used widely in architectures like MobileNet and Inception for reducing computational costs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3u2wsVCaUOG_"
      },
      "outputs": [],
      "source": [
        "class ResidualSep(nn.Module):\n",
        "    def __init__(self, channels, dilation=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            nn.ReflectionPad2d(dilation),\n",
        "            nn.Conv2d(channels, channels, kernel_size=3, stride=1,\n",
        "                      padding=0, dilation=dilation,\n",
        "                      groups=channels, bias=False),\n",
        "            nn.BatchNorm2d(channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(channels, channels, kernel_size=1, stride=1,\n",
        "                      padding=0, bias=False),\n",
        "            nn.BatchNorm2d(channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.blocks(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUNRYSjXUOHB"
      },
      "source": [
        "The residual hourglass-like blocks is a residual blocks with downsampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQkbf7ZpUOHB"
      },
      "outputs": [],
      "source": [
        "class ResidualHourglass(nn.Module):\n",
        "    def __init__(self, channels, mult=0.5):\n",
        "        super().__init__()\n",
        "\n",
        "        hidden_channels = int(channels * mult)\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            nn.ReLU(),\n",
        "            # Downsample\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(channels, hidden_channels, kernel_size=3, stride=2,\n",
        "                      padding=0, dilation=1,\n",
        "                      groups=1, bias=False),\n",
        "            nn.BatchNorm2d(hidden_channels),\n",
        "            # Bottleneck\n",
        "            ResidualSep(channels=hidden_channels, dilation=1),\n",
        "            ResidualSep(channels=hidden_channels, dilation=2),\n",
        "            ResidualSep(channels=hidden_channels, dilation=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(hidden_channels, channels, kernel_size=3, stride=1,\n",
        "                      padding=0, dilation=1,\n",
        "                      groups=1, bias=False),\n",
        "            nn.BatchNorm2d(channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            # Upsample\n",
        "            nn.ConvTranspose2d(channels, channels, kernel_size=2, stride=2,\n",
        "                               padding=0, groups=1, bias=True),\n",
        "            nn.BatchNorm2d(channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.blocks(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fZHbvUPvUOHD"
      },
      "source": [
        "Now we have all the blocks needed for building the model, and we can write a class for it. Note the normalization at the end of the initializer. We are going to pass image tensors with pixel values from a wide range so that we have to initialize the weights of the convolutions with smaller values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "019gRVGCUOHE"
      },
      "outputs": [],
      "source": [
        "class TransformerNet(torch.nn.Module):\n",
        "    def __init__(self, width=8):\n",
        "        super().__init__()\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(3, width, kernel_size=3, stride=1, padding=0, bias=False),\n",
        "            nn.BatchNorm2d(width, affine=True),\n",
        "            ResidualHourglass(channels=width),\n",
        "            ResidualHourglass(channels=width),\n",
        "            ResidualSep(channels=width, dilation=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(width, 3, kernel_size=3, stride=1, padding=1, bias=True)\n",
        "        )\n",
        "\n",
        "        # Normalization\n",
        "        self.blocks[1].weight.data /= 127.5\n",
        "        self.blocks[-1].weight.data *= 127.5 / 8\n",
        "        self.blocks[-1].bias.data.fill_(127.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.blocks(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDQk7OQdCXzW"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWjoI3e8wtm-"
      },
      "source": [
        "The class below specifies image loading and augmentations to add variety to our dataset and increase network stability to different input conditions.\n",
        "\n",
        "A dataset is an iterator-like object that returns an image and its label at every step. A loader is an object that aggregates the output of the dataset and returns batches.\n",
        "\n",
        "The dataset object is pretty simple. It loads an image from disk and applies the transforms to it. It returns a transformed image and the corresponding label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY6I-P5wCXzX"
      },
      "outputs": [],
      "source": [
        "class CocoDataset(data.Dataset):\n",
        "    def __init__(self, image_folder, image_list, augmentations):\n",
        "        self.image_folder = Path(image_folder)\n",
        "        self.image_list = image_list\n",
        "        self.augmentations = augmentations\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        image_id = self.image_list[item]\n",
        "        image_path = self.image_folder / (str(image_id).zfill(12) + '.jpg')\n",
        "        image = cv2.imread(str(image_path))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        scale = INPUT_HEIGHT / image.shape[0]\n",
        "        image = cv2.resize(image, None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
        "        augmented = self.augmentations(image=image)\n",
        "        image = augmented['image'].astype('float32')\n",
        "        return {'image': image.transpose(2, 0, 1)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsNX1Vi8UOHK"
      },
      "source": [
        "We will use augmentation to extend the dataset and increase its diversity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqMak-xgCXz9"
      },
      "outputs": [],
      "source": [
        "train_augmentations = albm.Compose([\n",
        "    albm.PadIfNeeded(min_height=INPUT_HEIGHT, min_width=INPUT_WIDTH, always_apply=True),\n",
        "    albm.RandomCrop(height=INPUT_HEIGHT, width=INPUT_WIDTH, always_apply=True),\n",
        "    albm.RandomBrightnessContrast(),\n",
        "    albm.HorizontalFlip()\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhfkw3T-UOHN"
      },
      "source": [
        "Let's create a dataset and a data loader objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpO-V4e5UOHN"
      },
      "outputs": [],
      "source": [
        "train_dataset = CocoDataset(COCO_IMAGES_PATH, train_img_ids,\n",
        "                            train_augmentations)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE,\n",
        "                                           num_workers=NUM_WORKERS, drop_last=True,\n",
        "                                           shuffle=True,\n",
        "                                           worker_init_fn=lambda _: np.random.seed())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Meq-gnA6UOHQ"
      },
      "source": [
        "For convenience let's create a function for displaying images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hK24Pm-UOHQ"
      },
      "outputs": [],
      "source": [
        "def show(img, nrow=8, interpolation='nearest'):\n",
        "    img = torch.clamp(img / 255.0, 0.0, 1.0)\n",
        "    img_grid = torchvision.utils.make_grid(img, nrow=nrow).numpy()\n",
        "    plt.figure(figsize=(12, 12))\n",
        "    plt.imshow(np.transpose(img_grid, (1, 2, 0)), interpolation=interpolation)\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpfPGIQKUOHV"
      },
      "source": [
        "Also we will use a function for loading an image and converting it to a PyTorch tensor that is ready to be passed into the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vbWKU6r5y-yX"
      },
      "outputs": [],
      "source": [
        "def image_to_batch(image_name):\n",
        "    image = cv2.imread(str(image_name))\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    scale = INPUT_HEIGHT / image.shape[0]\n",
        "    image = cv2.resize(image, None, fx=scale, fy=scale, interpolation=cv2.INTER_LINEAR)\n",
        "    image = image.astype('float32').transpose(2, 0, 1)\n",
        "    batch = torch.from_numpy(image).unsqueeze(0)\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVl-jjcsUOHZ"
      },
      "source": [
        "Let's test these two functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QttA97SZUOHZ"
      },
      "outputs": [],
      "source": [
        "style_img = image_to_batch(STYLE_IMAGE_PATH)\n",
        "style_img = style_img.to(device=DEVICE)\n",
        "show(style_img.cpu(), nrow=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAbZCIwXUOHc"
      },
      "source": [
        "# Auxiliary functions and models\n",
        "\n",
        "One of the most important parts of style transfer loss function is a so-called [Gram (of Gramian) matrix](https://en.wikipedia.org/wiki/Gramian_matrix). Its purpose is to remove spatial information remaining the style. You can find the details of the approach in the paper [*Johnson et al.* Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155).\n",
        "\n",
        "\n",
        "We can write functions for computing Gram matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nbCPiI7GUOHc"
      },
      "outputs": [],
      "source": [
        "def gram_matrix(y):\n",
        "    (bn, ch, h, w) = y.size()\n",
        "    features = y.transpose(0, 1).contiguous()\n",
        "    features = features.view(ch, w * h * bn)\n",
        "    features_t = features.transpose(0, 1)\n",
        "    gram = features.mm(features_t) / (ch * h * w * bn)\n",
        "    return gram"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPljV0SSUOHh"
      },
      "source": [
        "For computing the perceptual loss function we have to use a pretrained convolutional neural network. Following the papers on style transfer we will use pretrained VGG16."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPXc3q9lzddR"
      },
      "outputs": [],
      "source": [
        "class Vgg16(torch.nn.Module):\n",
        "    def __init__(self, requires_grad=False):\n",
        "        super().__init__()\n",
        "        vgg_pretrained_features = torchvision.models.vgg16(pretrained=True).features\n",
        "        self.slice1 = torch.nn.Sequential()\n",
        "        self.slice2 = torch.nn.Sequential()\n",
        "        self.slice3 = torch.nn.Sequential()\n",
        "        self.slice4 = torch.nn.Sequential()\n",
        "        for x in range(4):\n",
        "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(4, 9):\n",
        "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(9, 16):\n",
        "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
        "        for x in range(16, 23):\n",
        "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
        "        if not requires_grad:\n",
        "            for param in self.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        self.slices = [self.slice1, self.slice2, self.slice3, self.slice4]\n",
        "\n",
        "    def forward(self, x, features_num=4):\n",
        "        # Normalize according to VGG inputs\n",
        "        mean = x.new_tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
        "        std = x.new_tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
        "        h = (x / 255.0 - mean) / std\n",
        "\n",
        "        # Compute features\n",
        "        ret = []\n",
        "        for i in range(features_num):\n",
        "            h = self.slices[i](h)\n",
        "            ret.append(h)\n",
        "        return ret"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zdXRxn7UOHk"
      },
      "source": [
        "Let's initialize our feature extraction model and compute Gram matrix for style image features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBtT0c-7zwzx"
      },
      "outputs": [],
      "source": [
        "vgg = Vgg16(requires_grad=False).to(device=DEVICE)\n",
        "features_style = vgg(style_img)\n",
        "gram_style = [(gram_matrix(y).detach()) for y in features_style]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbhYKOEQCX0C"
      },
      "source": [
        "# Optimizer setup\n",
        "\n",
        "Here we set up training and model itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9zHT6C_CX0D"
      },
      "outputs": [],
      "source": [
        "model = TransformerNet(width=MODEL_WIDTH)\n",
        "model = model.to(device=DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsK7e4jKUOHq"
      },
      "source": [
        "We'll also set up a learning rate scheduler to drop learning rate if our network training process reaches a plateau."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u608-B2VUOHq"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',\n",
        "                                                       factor=LR_DECREASING_FACTOR, patience=2,\n",
        "                                                       verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AV1Yn35JwtnL"
      },
      "source": [
        "This helper function will display image and network output side by side to see the progress during training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ecTMXFrCX0J"
      },
      "outputs": [],
      "source": [
        "def show_test_image_quality(model, image):\n",
        "    scale = min(\n",
        "        INPUT_HEIGHT / image.shape[0],\n",
        "        INPUT_WIDTH / image.shape[1])\n",
        "\n",
        "    image = cv2.resize(image, None,\n",
        "                       fx=scale, fy=scale,\n",
        "                       interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    off_h = INPUT_HEIGHT - image.shape[0]\n",
        "    off_w = INPUT_WIDTH - image.shape[1]\n",
        "\n",
        "    model_input = torch.tensor(image.astype('float32').transpose(2, 0, 1))\n",
        "    model_input = F.pad(model_input, (off_w, 0, off_h, 0))\n",
        "\n",
        "    plt.subplot(121)\n",
        "    plt.imshow(image)\n",
        "    plt.axis('off')\n",
        "    plt.title('input')\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model_input = model_input.unsqueeze(0).to(DEVICE)\n",
        "        model_output = model(model_input)\n",
        "    output = model_output[0]\n",
        "    output = torch.clamp(output / 255.0, 0, 1)\n",
        "    output = output.cpu().numpy().transpose(1, 2, 0)\n",
        "    output = output[off_h:, off_w:, :]\n",
        "\n",
        "    plt.subplot(122)\n",
        "    plt.imshow(output)\n",
        "    plt.axis('off')\n",
        "    plt.title('output')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FeU4FgZdUOHw"
      },
      "source": [
        "Let's check the function with a fake identity model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4GZ4UWmEUOHw"
      },
      "outputs": [],
      "source": [
        "show_test_image_quality(lambda x: x, TEST_IMG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ3o5yG_CX0M"
      },
      "source": [
        "# Train loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aPSW0IEUOHz"
      },
      "source": [
        "For cycling the dataloader into an infinite batch generator we can use the following simple function. Then we can take from this generator any batches we need with standard `itertools.islice` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qrXa1M87UOHz"
      },
      "outputs": [],
      "source": [
        "def cycle(it):\n",
        "    while True:\n",
        "        yield from it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUqCunpfUOH3"
      },
      "source": [
        "Now we can run the main training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2W9MNhWsCX0N",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "train_loss = None\n",
        "ema_alpha = 0.05\n",
        "\n",
        "pbar = tqdm(islice(cycle(train_loader), NUM_TRAINING_STEPS),\n",
        "            dynamic_ncols=True, leave=False, mininterval=5, maxinterval=30,\n",
        "            desc='Training', total=NUM_TRAINING_STEPS)\n",
        "for batch_num, batch in enumerate(pbar):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    image = batch['image'].to(DEVICE)\n",
        "\n",
        "    noise = torch.zeros_like(image)\n",
        "    noise.normal_(mean=0, std=4)\n",
        "    styled_image = model(image + noise)\n",
        "\n",
        "    features_x = vgg(image, features_num=2)\n",
        "    features_y = vgg(styled_image)\n",
        "\n",
        "    content_loss = F.mse_loss(features_y[1], features_x[1])\n",
        "\n",
        "    # Compute style loss\n",
        "    style_loss = 0.\n",
        "    idx = 1\n",
        "    for ft_y, gm_s in zip(features_y, gram_style):\n",
        "        gm_y = gram_matrix(ft_y)\n",
        "        style_loss += F.mse_loss(gm_y, gm_s) * idx / gm_y.shape[1]\n",
        "        idx *= 2\n",
        "\n",
        "    # Compute consistency loss\n",
        "    dx = random.choice([-3,-2,-1,1,2,3])\n",
        "    dy = random.choice([-3,-2,-1,1,2,3])\n",
        "    image_shifted = torch.roll(image, shifts=(dx, dy), dims=(2, 3))\n",
        "\n",
        "    noise.normal_(mean=0, std=4)\n",
        "    styled_image_shifted = model(image_shifted + noise)\n",
        "    styled_image_shifted = torch.roll(styled_image_shifted, shifts=(-dx, -dy), dims=(2, 3))\n",
        "    consistency_loss = F.mse_loss(styled_image_shifted[:,:,4:-4,4:-4], styled_image[:,:,4:-4,4:-4])\n",
        "\n",
        "    loss_value = style_loss * STYLE_WEIGHT \\\n",
        "                 + content_loss * RECONSTRUCTION_WEIGHT \\\n",
        "                 + consistency_loss * CONSISTENCY_WEIGHT\n",
        "\n",
        "    pbar.set_postfix({\n",
        "        'Loss': f'{loss_value.item():.3f}',\n",
        "        'Style (×10⁶)': f'{style_loss.item() * 1e6:.3f}',\n",
        "        'Content': f'{content_loss.item():.3f}',\n",
        "        'Consistency': f'{consistency_loss.item():.3f}'\n",
        "    }, refresh=False)\n",
        "\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if train_loss is None:\n",
        "        train_loss = loss_value.item()\n",
        "    else:\n",
        "        train_loss = ema_alpha * loss_value.item() + (1 - ema_alpha) * train_loss\n",
        "\n",
        "    if batch_num % LOGGING_FREQUENCY == 0:\n",
        "        print(f'[{batch_num}] train \\t loss: {train_loss:.5f}')\n",
        "        scheduler.step(train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        show_test_image_quality(model, TEST_IMG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ojg_Gei6CX0S"
      },
      "source": [
        "# Export"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbZ1_i7kUqGx"
      },
      "source": [
        "It's time to convert our model to a universal format ONNX so that the model can be used in a lens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rMCKsFr0CX0S",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "dummy_input = torch.randn(1, 3, INPUT_HEIGHT, INPUT_WIDTH,\n",
        "                          dtype=torch.float32, device=DEVICE)\n",
        "\n",
        "output = model(dummy_input.detach())\n",
        "\n",
        "input_names = ['data']\n",
        "output_names = ['output1']\n",
        "\n",
        "torch.onnx.export(model, dummy_input,\n",
        "                  ONNX_PATH, verbose=False,\n",
        "                  input_names=input_names, output_names=output_names, opset_version=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQ_Ay8nzVgMA"
      },
      "source": [
        "If you use Google Colab you can download the model with the following code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD_HJAxIUOIA"
      },
      "outputs": [],
      "source": [
        "def download_onnx_from_colab():\n",
        "    from google.colab import files\n",
        "    files.download(ONNX_PATH)\n",
        "\n",
        "## Uncomment for downloading ONNX from Colab\n",
        "# download_onnx_from_colab()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "style_transfer.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "87a1373ae70b4cb3a09f44b2ad35d22a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82a7f264a43b475d9c70750d0f7cf38f",
              "IPY_MODEL_1f6c859a79d8400c8d4bfa51985c2df0",
              "IPY_MODEL_79b5e8c093124858ab43b24d69493170"
            ],
            "layout": "IPY_MODEL_b7057dbfc7f542e0be08a06ac95b5378"
          }
        },
        "82a7f264a43b475d9c70750d0f7cf38f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8156670519649f3b34814769881f4d1",
            "placeholder": "​",
            "style": "IPY_MODEL_a098227217dc4e4fabd8807980d3df2e",
            "value": "  0%"
          }
        },
        "1f6c859a79d8400c8d4bfa51985c2df0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6b03af9b0014de9a7e981ad391b8a08",
            "max": 32000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f85be314480419aa274e8f955e695c2",
            "value": 32000
          }
        },
        "79b5e8c093124858ab43b24d69493170": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f0a23b63bc84e8a8c593a8c6d4f791f",
            "placeholder": "​",
            "style": "IPY_MODEL_7762b5a6a7f64c72b5b4fbb7d3f3af1e",
            "value": " 0/32000 [00:00&lt;?, ?it/s]"
          }
        },
        "b7057dbfc7f542e0be08a06ac95b5378": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": "100%"
          }
        },
        "b8156670519649f3b34814769881f4d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a098227217dc4e4fabd8807980d3df2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6b03af9b0014de9a7e981ad391b8a08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f85be314480419aa274e8f955e695c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f0a23b63bc84e8a8c593a8c6d4f791f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7762b5a6a7f64c72b5b4fbb7d3f3af1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}